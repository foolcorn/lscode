### 连接池

对数据库进行连接，单个连接处理所有业务，可能业务A阻塞在io上，B会因为A的io阻塞而阻塞，事实上两者完全可以并行，只要不是同时写入，一个读一个写，或者两个读都可以



池化技术将连接用容器管理

新建连接

请求连接

归还连接



一般情况下线程数量和连接数量保持一致



### 线程池



有几种设计模式

- recv，parser，send三个步骤都放到主线程里执行，适合在内存就能完成的业务，不涉及对硬盘的读写（日志）
- 如果parser比较复杂，可以把recv放在主线程，然后把parser，send放到线程里执行
- recv，parser，send三个步骤都放到线程里执行，速度最快，但是有个致命问题是，多个线程共用一个fd，假设发起的请求间隔时间短，socket同时被两个线程在处理



组成

- 任务队列
- 执行队列
- 管理组件：
  - 互斥锁
  - 条件变量

任务队列类{callback，data，next，prev} 

执行队列{线程id，canused优雅指示当前线程是否可用，next，prev}

管理类（线程池）{mutex cond，任务队列，执行队列}

泛型添加节点

泛型删除节点



api

创建线程池（）

pushtask，给线程池添加任务

销毁线程池



性能分析思路：加入线程池比不加线程池效率提升6倍



后期优化点：

- 怎么解决多个线程共用一个fd：协程
- 增加一个job_count计算任务队列数量：根据任务队列和执行队列的比例，动态调整线程池的线程数量，保持一个40%~60%的使用量



### 内存池

如果业务对于客户端的每个请求，都需要使用malloc，一旦频繁使用malloc分配小内存，就容易导致内存碎片

对于小内存，就可以使用内存池的方式进行缓冲



组件：

小内存块（4k）（包含很多碎片）：  

- last：当前分配的碎片结尾

- end：内存最后一个碎片结尾
- next：指向下一个内存块



大内存块（大于4k）：

- next：指向下一个大内存块
- ptr：指向大内存



内存池：

- 大块头指针 head
- 小块头指针 large = null
- 小块当前指针current



有个点比较拗口，就是说内存池的数据结构本身也用内存池来存储(原汤化原食)

api：

创建内存池pool：create(size)

new(p) sizeof pool+size//-这样相当于分配size的前面有一个pool结构体

current = head = p+1





销毁内存池



分配 p = alloc(pool, size)





释放free(pool,p)



接触陌生的服务进程，用htop发现虚拟内存在涨，怎么解决

- 判断内存池是否有内存泄漏：使用内存池在分配和销毁的时候加上打印信息来排错
- 第三方的内存库，是否会有内存泄漏：在哪个业务场景里出现，慢慢排除



 

### 请求池

 

请求池是针对异步请求的

多个请求对应多个fd用epoll管理，关注可读事件

异步：请求与响应不会在一个线程里。

 

1. 建立请求池中的连接，将fd加入到epoll
2. 异步操作上下文。开多个线程去query请求mysql
3. epollwait可读事件，并recv后，解析协议，进行对应操作
4. 销毁fd和线程



同步是串行的，要等待结果

异步是并行的，只管发送请求，不用等结果

 

异步上下文：epollfd，线程tid

异步dns代码：

- init：初始化，开epoll，开一个主线程，主线程拥有epollfd

- 主线程函数：主体死循环，epoll wait这个epollfd管理的所有事件，wait到以后，遍历每个活跃的socket，并recv udp报文进行协议的解析，之后调用epollevent中包含的回调对解析后的结果进行业务处理
- commit：建立socket，设置目标的ip和端口，connet socket，并提交请求，之后将该socket加入到epoll树中，附带上最后业务处理的回调。
- 业务处理完以后:改socketfd完成自己的使命（接受某个ip地址的报文）后，可以重用，怎么存放该socketfd?，还是放到epoll里，关注事件变成epollout，同时设置成边缘触发模式ET，设置一个计时器，防止该socketfd长久不使用。 

- 有个注意点，如果epoll wait不到怎么办，udp是不可靠的，怎么判断重发？可以每个fd都要加定时器，超时就重新 commit，也可以把计时器也变成fd加入到epoll中（没搞懂）



有点牛啊，感觉在客户端搭建了一个epoll服务器，可以串行地解决并行的请求，相当于客户端自己变成了很多个客户端，然后客户端获取服务端的epoll串行结果。

epoll不是基于连接的，而是基于fd的，所以udp也可以用

 

定时器数据结构



接口：

init timer

add timer（ptr,callback）

delete timer

find nearest timer

update timer 定时更新检测失效



底层数据结构：

- 红黑树（nginx）增删查都是不标准log2n
- 小顶堆（go）：增查是标准log2n（比红黑树更平衡），删是on，通过hashtable可以优化到o1
  - 小顶堆删除某个节点，通过map直接找到index，与-1元素对调，删除-1元素，然后fix
- 跳表：
- 时间轮：（linux）



