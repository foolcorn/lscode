### udp怎么做并发服务器

recvfrom(fd,addr,buffer,length, 0)

在recvfrom的时候对数据包进行处理





### socketfd的理解：

插座：fd--->(sip,dip,sport,dport,protocol)每个fd由一个五元组唯一标识

可以当做是文件描述符，也可以当做是一个客户端

我的理解是一段远程的缓冲区。



sighand_struct来存信号

每个进程有个sigaction有个64位的action来存信号

  

select(集合长度, 集合1，集合2，集合3，多长时间一次)；

select(ionum, rfds,wfds,efds,timeout)，可写集合，可读集合，根据timeout定时轮询一次

io数量  fd是int，取maxfd+1？



poll(pfd,length,timeout)



epoll



调用系统io都要经历两个阶段，

- 等待内核空间数据准备就绪
- 将数据从内核态拷贝到用户态



### 同步io：

阻塞io：如果调用io接口的时候，内核还没有准备好数据，此时进程，线程会阻塞直到等到数据并拷贝到用户，内核返回本次操作的结果，此时经历了两个阶段才会解除阻塞

缺点：等待数据的时候，进程会被阻塞，无法执行任何运算，和接收网路请求

解决办法：多线程，使用并发，当一个线程阻塞，另一个线程立马接手，充分利用cpu，然而线程的数量是有上限的，依旧无法处理大并发，线程池也只是治标不治本的缓解措施。



非阻塞io：如果调用io的时候，数据没有准备好了，会立马返回没有准备好的结果，不会让进程阻塞，因此用户必须一直循环调用该io直到完成读取任务。依然无法解决大并发问题。



多路复用io：将两阶段的第一阶段单独抽离出来作为一个查询就绪状态的api，其实就相当于一个非阻塞io，在一个进程里面可以对大量的fd进行就绪状态的查询，然后对所有就绪的fd进行读写操作。



如果不是做大并发服务器，使用阻塞io加多线程反而比多路复用有更好的性能（展示）。



select接口

![image-20220620175337058](C:\Users\37412\AppData\Roaming\Typora\typora-user-images\image-20220620175337058.png)

fd_set是位图，所以最大只能存1024个文件描述符

每次select返回的集合，都是位图，比如read集合，哪个位置一说明该fd为可读状态。

epoll是不跨平台的，不同操作系统的epoll实现有很大差异。



其次，原生的select和epoll等多路复用，有个问题，因为处理每个活跃的fd都是按时序来处理的，如果对于每个活跃的socket的处理程序过于庞大和耗时，会导致处理一个socketfd过久，而后续的socketfd迟迟无法响应，所以需要更大的框架（libevent，libevent的替代品libev），对于不同的epoll事件，选择不同的线程来处理，加入信号技术支持异步io响应。

事实上linux现在也支持了异步io接口，aio_read,aio_write



### 异步io

进程调用aio_read后，如果第一阶段没有准备好，会直接返回，进程可以直接进行后续的流程，不需要一直循环询问是否准备好了，而是等到第二阶段完成后，内核会发送完成信号中断进程来处理该io事件。

非阻塞io虽然在第一个阶段没准备好的时候能直接返回，但是，当第一个阶段完成后，再次循环调用该io，进程会阻塞，直到第二阶段拷贝完成，而异步io是连第二阶段也不会阻塞，直到第二阶段完成后才由信号进行通知。



信号驱动io

利用SIGIO信号：网卡收到数据后通知内核给进程发送的信号

使用signal或者sigaction绑定handler：

但是sigio不能用到tcp上面（理论上可行，但是实际不行）

tcp的触发的频率会比udp大太多，所以会一直被中断，于是实现上，tcp不触发sigio信号。



### Reactor

等待消息



epoll_wait的epoll event数组的大小一般可以取最大连接数的百分之一

内核代码：fs/eventpoll.c



对于读事件

LT:水平触发，recvbuffer有数据就会一直触发，虽然是活跃的，当然还是得等到下一个循环才能继续读。

ET：边缘触发，当recvbuffer从空变有数据的时候才会触发。



什么时候用ET什么时候用LT？

其实若要完整地读取缓冲区里的数据，ET和LT都可以

- 因为ET只触发一次，所以可以在recv的时候，外面套一个while循环，一直读到-1为止
- LT可以触发多次，所以只需要每次读一点，下次外面的大while循环再次进入的时候会接着读，直到该socketfd不活跃为止。



但是，如果数据量特别大，依旧建议用LT，因为假设ET传的数据特别大，又有while循环，相当于该fd之后的所有fd都被这个io给阻塞了，而后续的fd只需要读很小的数据量就可以完成事件处理，却要等当前的fd很久，就很不划算，反而LT每次只读一点就让给下一个fd，自己则等下一个while循环再读取，会更合理。

而如果读取的都是小数据的话，则可以用ET



### epoll 改写

- 原生epll框架，epoll数组存listenfd，后续存clientfd

  每次判断是不是listenfd或者是clientfd

- 不用判断listenfd，而是利用event.data.ptr存回调函数，在epollctl的时候就给每个fd绑定上对应的处理

  此时只用处理eppllin事件和epollout事件

- 如果收到读数据后想要直接写数据该怎么办，在读回调的时候，使用epollctl不关注可读状态，而关注可写状态，在写回调的时候，则继续用epollctl把感兴趣的事件再改回关注可读事件

- 继续改写，把epollfd和epollevent数组抽离出来封装成类reactor，对象可以命名成eventloop。在ptr指向的结构体里面存sendbuffer和recvbuffer，解决tcp半包问题。



为什么要用reactor，不仅仅是面向对象，同时要业务处理从io循环里剥离出去，主要就是通过开线程的方式，如果业务和io在同一个线程里面，而一个业务处理特别慢，就会阻塞后面的所有io。



单线程reactor：libevent，redis

多进程reactor，因为每个accept需要数据共享，可以用一个后台缓存数据库来存储运行状态，或者说session，上下文，每个进程只处理业务，不存储数据，例子：nginx

多线程reactor，多线程本就支持数据共享，但是需要谨慎加锁解锁，实现方式：多个workers，或者一个accept master+多个workers，例子：memcached



多进程监听一个端口，在listen之后fork，子进程继承了父进程的listenfd，但是这样每个进程都有listenfd，如果一个请求到来，由哪个进程来处理？这就是惊群问题。解决办法;进程间通信，保持每个时刻只有一个listenfd在epollevent数组里，通过锁机制来控制独占。



http1.0请求类型：

get

post

head：请求报头，204no content

http1.1新增

put：客户端像服务器传送数据取代指定文档内容

detete：请求服务器删除指定页面

connect：管道方式

options：客户端查看服务器性能

trace：回显服务器收到的请求

patch：对put方法的补充



http字段：

last-modified：文档最后改动时间



http状态码：

100 continue 意味着让客户端继续其请求

101 switching protocols 切换协议，服务器根据客户端请求切换协议



201 created 已创建，成功请求并创建了新的资源

201 accepted：已经接受请求，但未处理完成

203 非授权信息。请求成功，但返回的meta信息不在原服务器，而是一个副本

205 reset content 重置内容

206 partial content 部分内容



300 multiple choices 多种选择，请求的资源包括多个位置

305 use proxy 使用代理



401 unauthorized 请求要求用户的身份认证。

405 method not allow 哭护短请求中的方法被禁止

406 服务器无法根据客户端请求的内容完成请求



503 service unavailable 由于超载或系统维护，服务器暂时无法处理客户端的请求，网络服务正忙，请稍后重试。



websocket：服务器主动发送信息给客户端

http:客户端主动发送请求数据



tcp虽然有保活机制，但是一般不用，而是用应用层的心跳包来更好的控制。





